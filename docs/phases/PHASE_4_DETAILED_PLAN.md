# Phase 4: Application Services - Detailed Implementation Plan

## Overview

Phase 4 focuses on implementing the application layer that orchestrates between the presentation layer (API) and the domain/infrastructure layers. This phase builds upon the solid infrastructure foundation from Phase 3 to deliver the core business logic and SEC filing analysis capabilities of Aperilex.

## Architecture Principles

### Clean Architecture Layers
- **Presentation Layer**: HTTP/REST concerns only (FastAPI)
- **Application Layer**: Use cases, orchestration, and workflow coordination
- **Domain Layer**: Business entities, rules, and domain services
- **Infrastructure Layer**: External integrations and data persistence

### Design Patterns
- **CQRS Pattern**: Separate commands (write) and queries (read) for clarity
- **Command Pattern**: Encapsulate requests as objects with validation
- **Repository Pattern**: Abstract data access (already implemented)
- **Dependency Injection**: Constructor injection for all services
- **Domain-Driven Design**: Rich domain models with business logic

## Implementation Components

### 1. Base Command/Query Infrastructure

#### Command Base Classes
- `BaseCommand`: Abstract base for all commands with metadata
- `BaseCommandHandler[TCommand, TResult]`: Generic handler interface
- Command validation using Pydantic models
- Command metadata: timestamp, correlation_id, user_context

#### Query Base Classes  
- `BaseQuery`: Abstract base for all queries
- `BaseQueryHandler[TQuery, TResult]`: Generic handler interface
- Query validation and parameter constraints
- Result pagination support for list queries

#### Handler Registration
- Command/Query dispatcher for routing
- Dependency injection container setup
- Handler lifecycle management
- Error handling and logging middleware

### 2. Analysis Use Cases

#### AnalyzeFilingCommand
**Purpose**: Trigger comprehensive analysis on a specific SEC filing

**Components**:
- Command with filing identification (CIK, accession number)
- Analysis parameters (template, depth, focus areas)
- Handler orchestrating the analysis workflow
- Integration with EdgarService and LLM providers
- Background task creation for long-running analysis
- Result persistence and cache management

**Workflow**:
1. Validate filing exists and is accessible
2. Fetch filing content via EdgarService
3. Apply analysis template or default configuration
4. Submit to LLM provider for analysis
5. Store results in Analysis repository
6. Update filing status to analyzed
7. Trigger any configured notifications

#### GenerateInsightsCommand
**Purpose**: Derive higher-level insights from multiple analyses

**Components**:
- Command with analysis criteria (companies, time period, metrics)
- Insight generation strategies (trending, comparison, anomaly)
- Handler coordinating cross-analysis processing
- Integration with InsightGenerator domain service
- Batch processing for large datasets
- Result aggregation and ranking

**Workflow**:
1. Query relevant analyses based on criteria
2. Load analysis results into memory
3. Apply insight generation algorithms
4. Rank and filter insights by relevance
5. Store derived insights with provenance
6. Generate summary report

#### CompareAnalysesQuery
**Purpose**: Compare analysis results across companies or time periods

**Components**:
- Query with comparison parameters
- Comparison dimensions (metrics, time, peer group)
- Handler fetching and aligning data
- Result formatting for presentation
- Caching strategy for repeated comparisons

**Workflow**:
1. Validate comparison parameters
2. Fetch relevant analyses from repository
3. Normalize data for comparison
4. Calculate differences and trends
5. Format results with visualization hints
6. Cache results for performance

### 3. Domain Services

#### AnalysisOrchestrator
**Purpose**: Coordinate complex multi-step analysis workflows

**Responsibilities**:
- Workflow definition and execution
- Step sequencing and parallelization
- Error handling and retry logic
- Progress tracking and reporting
- Resource management (rate limiting)

**Key Methods**:
- `orchestrate_filing_analysis()`: Full filing analysis workflow
- `orchestrate_batch_analysis()`: Multiple filings in parallel
- `handle_analysis_failure()`: Failure recovery strategies
- `get_workflow_status()`: Progress monitoring

#### InsightGenerator
**Purpose**: Extract actionable insights from analysis results

**Responsibilities**:
- Pattern recognition across analyses
- Trend identification and forecasting
- Anomaly detection algorithms
- Insight ranking and prioritization
- Natural language generation for insights

**Key Methods**:
- `generate_insights()`: Main insight generation
- `identify_trends()`: Time-series analysis
- `detect_anomalies()`: Statistical outlier detection
- `rank_insights()`: Relevance scoring
- `format_insights()`: Human-readable output

#### AnalysisTemplateService
**Purpose**: Manage reusable analysis configurations

**Responsibilities**:
- Template CRUD operations
- Template validation and versioning
- Default template management
- Template sharing and permissions
- Template performance optimization

**Key Methods**:
- `create_template()`: New template creation
- `get_template()`: Retrieve by ID or name
- `update_template()`: Modify existing
- `list_templates()`: Query available templates
- `apply_template()`: Configure analysis with template

### 4. API Endpoints

#### Filing Endpoints
- `POST /filings/analyze`: Trigger filing analysis
- `GET /filings`: List filings with filters
- `GET /filings/{filing_id}`: Get filing details
- `GET /filings/{filing_id}/analysis`: Get analysis results
- `POST /filings/batch-analyze`: Analyze multiple filings

#### Company Endpoints
- `GET /companies`: List companies
- `GET /companies/{company_id}`: Get company details
- `GET /companies/{company_id}/filings`: Get company filings
- `GET /companies/{company_id}/analyses`: Get all analyses
- `POST /companies/{company_id}/analyze-latest`: Analyze latest filing

#### Analysis Endpoints
- `GET /analyses`: List analyses with filters
- `GET /analyses/{analysis_id}`: Get analysis details
- `POST /analyses/compare`: Compare multiple analyses
- `POST /analyses/generate-insights`: Generate insights
- `GET /analyses/templates`: List analysis templates
- `POST /analyses/templates`: Create new template

#### System Endpoints
- `GET /tasks/{task_id}`: Check background task status
- `GET /health/ready`: Readiness check
- `GET /health/live`: Liveness check
- `GET /metrics`: Prometheus metrics

### 5. Request/Response Schemas

#### Request DTOs
- `AnalyzeFilingRequest`: Filing analysis parameters
- `CompareAnalysesRequest`: Comparison criteria
- `GenerateInsightsRequest`: Insight generation params
- `PaginationParams`: Standard pagination
- `FilterParams`: Standard filtering

#### Response DTOs
- `FilingResponse`: Filing details with metadata
- `AnalysisResponse`: Analysis results with scores
- `InsightResponse`: Generated insights
- `ComparisonResponse`: Comparison results
- `TaskResponse`: Background task status
- `ErrorResponse`: Standardized error format

### 6. Integration Patterns

#### Background Task Integration
- Celery task creation for long operations
- Task status tracking and updates
- Result retrieval patterns
- Failure handling and retries
- Task cancellation support

#### Cache Integration
- Response caching for read endpoints
- Cache key strategies by domain
- Cache invalidation on mutations
- Cache warming for common queries
- TTL configuration by data type

#### External Service Integration
- EdgarService async wrapper patterns
- LLM provider abstraction usage
- Rate limiting and backoff strategies
- Circuit breaker implementation
- Monitoring and alerting hooks

## Testing Strategy

### Unit Tests
- Command/Query handler logic
- Domain service algorithms
- Schema validation
- Business rule enforcement
- Error handling paths

### Integration Tests
- API endpoint functionality
- Database transaction handling
- Cache behavior verification
- Background task execution
- External service mocking

### End-to-End Tests
- Complete analysis workflows
- Multi-step use cases
- Performance benchmarks
- Concurrent request handling
- Error recovery scenarios

## Quality Assurance

### Code Quality
- MyPy strict type checking
- Ruff linting compliance
- Black/isort formatting
- Test coverage > 90%
- Documentation completeness

### Performance Considerations
- Async/await throughout
- Database query optimization
- Efficient cache utilization
- Batch processing for scale
- Resource pooling

### Security Measures
- Input validation via Pydantic
- SQL injection prevention
- Rate limiting per endpoint
- Request size limits
- Audit logging

## Success Criteria

1. All use cases implemented with full test coverage
2. API endpoints functional with proper documentation
3. Background task processing working reliably
4. Cache integration improving performance
5. Domain services providing business value
6. Clean separation of concerns maintained
7. Type safety enforced throughout
8. All existing tests continue to pass

## Dependencies

- Phase 3 infrastructure components
- EdgarTools library for SEC data
- OpenAI API for analysis
- PostgreSQL for persistence
- Redis for caching
- Celery for background tasks
- FastAPI for REST endpoints

## Next Steps

After Phase 4 completion:
- Phase 5: REST API & Frontend
- Phase 6: Authentication & Security
- Phase 7: Production Readiness